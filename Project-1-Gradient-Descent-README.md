ğŸŒŸ Gradient Descent Visualizer

A simple mathematical demonstration of how machines â€œlearnâ€ by reducing error.

ğŸ” What This Project Shows

This project visualizes gradient descent, the fundamental optimization algorithm used in:

machine learning

deep learning

AI systems

neural networks

It teaches how a â€œmodelâ€ walks downhill on a curve to find the minimum error.

ğŸ§  Concept (5-year-old friendly)

Imagine you are on a hill in fog.

You canâ€™t see the top or bottom.

You feel the ground:

If it goes upward â†’ step the opposite direction

If it goes downward â†’ follow it

After many small steps, you reach the lowest point.

This is exactly how AI learns.

ğŸ“ˆ Mathematics Behind It

We minimize this function:

f(x) = xÂ² + 3x + 2


Its derivative:

fâ€™(x) = 2x + 3


Gradient descent update rule:

x_new = x_old â€“ learning_rate * fâ€™(x_old)


The model keeps moving until slope â‰ˆ 0.

ğŸ§ª What the Code Does

âœ” Plots the curve
âœ” Computes derivative
âœ” Takes 25 steps downhill
âœ” Shows the â€œwalking pathâ€ with red dots
âœ” Prints the minimum found

ğŸ§˜â€â™‚ï¸ Why This Project Matters (for PhD)

This project demonstrates:

understanding of optimization

ability to implement mathematical algorithms

ability to visualize learning dynamics

interest in mathematical foundations of machine learning

This is exactly what professors in applied math + ML want to see.
